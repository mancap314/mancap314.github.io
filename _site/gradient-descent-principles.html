<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title> Gradient Descent Explained</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="favicon.png">
    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script type="text/javascript" src="assets/js/switch.js"></script>
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Gradient Descent Explained | Data science and other stuffs</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Gradient Descent Explained" />
<meta name="author" content="Manuel Capel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Gradient descent is a major technique for training ML/DL models. Let’s have a closer look at it and implement a simple example from scratch in Python illustrating the main basic concepts around gradient descent." />
<meta property="og:description" content="Gradient descent is a major technique for training ML/DL models. Let’s have a closer look at it and implement a simple example from scratch in Python illustrating the main basic concepts around gradient descent." />
<link rel="canonical" href="https://mancap314.github.io/gradient-descent-principles.html" />
<meta property="og:url" content="https://mancap314.github.io/gradient-descent-principles.html" />
<meta property="og:site_name" content="Data science and other stuffs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-02T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Gradient Descent Explained" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Manuel Capel"},"dateModified":"2020-08-02T00:00:00+02:00","datePublished":"2020-08-02T00:00:00+02:00","description":"Gradient descent is a major technique for training ML/DL models. Let’s have a closer look at it and implement a simple example from scratch in Python illustrating the main basic concepts around gradient descent.","headline":"Gradient Descent Explained","mainEntityOfPage":{"@type":"WebPage","@id":"https://mancap314.github.io/gradient-descent-principles.html"},"url":"https://mancap314.github.io/gradient-descent-principles.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-135039700-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-135039700-1');
    </script>
  </head>
  <body>
    <nav class="navigation">
  <div>
    <p><em>Personal nerdy stuffs</em></p>
    
      <a href="/" >Home</a>
    
      <a href="/about.html" >About</a>
    
      <a href="/contact.html" >Contact</a>
    
  </div>
</nav>
<div class="spacer-top">
  &nbsp;
</div>

    <div class="main-content">
      <h1>Gradient Descent Explained</h1>
      
<div class="post-desc horizontal-list">
  <ul>
    <li>02 Aug 2020</li>
    <li>Manuel Capel</li>
    <li>Tags: <a href="/machine-learning/">machine learning</a>&nbsp;<a href="/deep-learning/">deep learning</a></li>
  </ul>
</div>

      <p>Gradient descent is a major technique for training ML/DL models. Let’s have a closer look at it and implement a simple example from scratch in Python illustrating the main basic concepts around gradient descent.</p>

<p>Gradient descent is omnipresent in the training of neural networks and other machine learning models. Each time you have a model which parameters are updated during the training according to a loss function, you can be almost certain that there is gradient descent under the hood. Here we will see how gradient descent works along a very simple linear regression example.</p>

<p>But before starting, what is a gradient and why descend it?</p>

<h2 id="gradient-descent-graphically">Gradient descent, graphically</h2>

<p>Imagine you have to approximate a dataset \((X, y)\) where \(X\) represents the features of your datapoints and \(y\) the target values (to predict) on those datapoints. The goal is to approximate the relationship between \(X\) and \(y\) by a function \(f\) depending on a parameter (weight) \(w\), for example \(f_w: x →  w.x\). So you want to find the value of \(w\) making \(f_w\) the closest to \(y\). But what does the closest means? For this, we use an error (also called cost) function \(C\). On a given datapoint \(x\), we compute \(C(f_w, x, y)\), also called loss. The higher the loss, the worse \(f_w\) approximates the data on \(x\), so we have to adjust \(w\). In machine learning, \(f\) would be the model and training the model \(f\) means finding the parameter \(w\) minimizing \(C(f_w, x, y)\) for the datapoints \(x\) we have in the training dataset.</p>

<p>So you take a value for \(w\), you compute \(f_w(x)\), you compare it with the corresponding \(y\) from your training dataset. How do you use this information to adjust \(w\) in order to reduce this error?</p>

<p><img src="assets/gradient-descent-step.png" alt="gradient descent step illustrated" title="Gradient Descent Step Illlustrated" /></p>

<p>Usually the gradient descent is represented visually by small arrows going down the cost function, but this is only part of the story. The <em>gradient</em> is in fact the “slope” of the vector orthogonal to the <em>tangent hyperplane</em>. Let’s first have a short glance at the <em>tangent hyperplane</em>.</p>

<p><strong>Tangent hyperplane</strong>: <em>tangent</em> comes from the Latin and means “touching”. Here the <em>tangent</em> is <em>touching</em> the cost function at the point corresponding to the current parameter. A <em>hyperplane</em> is a plane of dimension \(N-1\) in a space of dimension \(N\). In this case the parameter \(w\) is 1-dimensional, so the space is 2-dimensional, means it has 2 axis: one for the parameter value, one for the error (cost) value. If w would be 2-dimensional, then the space would be 3-dimensional (2 dimensions for w and 1 for the error), the cost function would be a curved surface in this space, and the tangent hyperplane would be a (3–1 = 2)-dimensional plane touching this curve at this point.</p>

<p>The <em>gradient vector</em> is the vector generating the line orthogonal to the tangent hyperplane. Then you take the opposite of this vector (hence “descent”), multiply it by the <em>learning rate</em> \(lr\). You get the green vector on the image above. The projection of this vector on the parameter space (here: the x-axis) gives you the new (updated) parameter. Then you repeat this operation several times to go down the cost (error) function, with the goal of reaching a value for w where the cost function is minimal. The parameter is thus updated as follow at each step:</p>

\[parameter \leftarrow parameter - lr.gradient\]

<p><strong>Intuition</strong>: Why is this method used, why does it work? Well, imagine \(w = (w_1, w_2)\) is 2-dimensional. You test of value of \(w\), let say \((3, 2)\) and then see this value is quite wrong. How to adjust it? Increasing \(w_1\) and decreasing \(w_2\), or this opposite, or increasing both? By how much? You should test then many parameter values around \((3, 2)\) to have an approximate map of the cost function around this point, just to know in what direction to go, then do the same at the new place… it would be extremely computationally intensive, especially if w is like many-millions-dimensional. The gradient is very powerful: for a parameter value, you just have to compute the corresponding value and gradient of the cost function, and you know where to go.</p>

<p>The thing is, the gradient tells you only the direction, not the distance to go in that direction. If the learning rate is to high, you may go to far in that direction, and possibly completely miss a minimum. Or the learning rate is too small, and you make 10 small steps where one big step would have been enough, thus training your model way too slowly. Finding, optimizing and adapting the learning rate is the topic of ongoing researches for many years, the work done in this area would fill many big books. If you want to go further, this paper for example benchmarks various learning rate policies that you can research if you’re interested.</p>

<p>In the following, we will see how to implement gradient descent and its main variants on a simple example: finding the optimal slope for a 1-dimensional linear regression.</p>

<h2 id="example-setting">Example setting</h2>

<p>We generate <code class="language-plaintext highlighter-rouge">N=150</code> points following the distribution \(Y \sim a.X + ε\) where \(ε \sim N(0, 10)\) is a Gaussian white noise, in order to satisfy linear regression conditions. Here <code class="language-plaintext highlighter-rouge">a = 3.2</code> is the true value to reach. We will start with w=10 and then try to make it reach 3.2 through gradient descent.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">150</span>  <span class="c1"># `N` random numbers between 0 and 100
</span><span class="n">a</span> <span class="o">=</span> <span class="mf">3.2</span>
<span class="n">w_start</span> <span class="o">=</span> <span class="mi">10</span><span class="n">X</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># Generate y = a * x + eps
# eps: 0-centered Gaussian distributed
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true line'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_start</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> 
            <span class="n">c</span><span class="o">=</span><span class="s">'orangered'</span><span class="p">,</span> 
            <span class="n">linestyle</span><span class="o">=</span><span class="s">'dotted'</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s">'start line'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="assets/gradient-descent-first-example.png" alt="gradient-descent-first-example" title="Starting from the dashed line, approaching the plain line. True model underlying the blue datapoints through gradient descent…" /></p>

<p><em>Goal</em>: make the start line reach the true line through gradient descent.</p>

<p>Following the formalism above:</p>
<ul>
  <li>\(f_w : x → w.x\) represented by the dotted line</li>
  <li>\(w\): parameter (weight) of the model f representing its slope</li>
  <li>\(C : (f_w, x, y) → (f_w(x) — y)²\) the cost function. It’s simply the square of the difference between the value given by the model at \(x\) (means \(wx\)) and the true value \(y\). Its gradient is given by the partial derivative of \(C(f_w, x, y)\) by \(w\), means here \(2x(wx — y)\).</li>
</ul>

<h2 id="applying-gradient-descent">Applying gradient descent</h2>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>

<p>Here we go point by point through the data. At each point, we compute the value given by the current model, the corresponding error, and adjust it by gradient descent. Remember there is another ingredient in the gradient descent, the <em>learning rate</em>, determining how far we go in the direction given by the gradient at each step. So we will do this for three difference learning rates: 0.0001, 0.001 and 0,01 and compare the results.</p>

<p>For this, we define a function <code class="language-plaintext highlighter-rouge">gradient_descent_linreg()</code> iterating through all the points of the <code class="language-plaintext highlighter-rouge">(X, Y)</code> sample correcting each time <code class="language-plaintext highlighter-rouge">w</code> according to the loss and learning rate <code class="language-plaintext highlighter-rouge">lr</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">gradient_descent_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">ws</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="p">)</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># gradient
</span>        <span class="c1"># adjust the parameter by gradient descent:
</span>        <span class="n">w</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">ws</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s">'w'</span><span class="p">:</span> <span class="n">ws</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">:</span> <span class="n">losses</span><span class="p">}</span>

<span class="n">descents</span> <span class="o">=</span> <span class="p">{</span><span class="n">lr</span><span class="p">:</span> <span class="n">gradient_descent_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> \
                    <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="p">[.</span><span class="mi">0001</span><span class="p">,</span> <span class="p">.</span><span class="mi">001</span><span class="p">,</span> <span class="p">.</span><span class="mi">01</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">plot_descents_lr</span><span class="p">(</span><span class="n">descents</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'Blues'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">descents</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">descents</span><span class="p">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">))),</span> <span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">),</span> 
                    <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> 
                    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'lr=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">))</span> \
                    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">descents</span><span class="p">.</span><span class="n">values</span><span class="p">()])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'True Value'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'iteration'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'w'</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s">'Estimation of w by Iteration'</span>\
            <span class="s">'for Various Learning Rates'</span>
    <span class="k">if</span> <span class="n">batch_size</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="se">\n</span><span class="s">Batch Size = </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">'</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">descents</span><span class="p">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'loss'</span><span class="p">))),</span> 
                    <span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'loss'</span><span class="p">),</span> 
                    <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> 
                    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'lr=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'iteration'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'Estimation'</span><span class="p">,</span> <span class="s">'Loss'</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">plot_descents_lr</span><span class="p">(</span><span class="n">descents</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="assets/gradient-descent-by-lr.png" alt="gradient-descent-by-lr" title="parameter value w and loss by step for various learning rates" /></p>

<p>Here we see some interesting things:</p>
<ul>
  <li>the smallest <em>learning rate</em> 0.0001 is too small and thus takes too long to make the parameter <code class="language-plaintext highlighter-rouge">w</code> reach the true value</li>
  <li>the biggest <em>learning rate</em> 0.01 brings <code class="language-plaintext highlighter-rouge">w</code> very fast to the true value but then oscillates. Being a big step size, it makes <code class="language-plaintext highlighter-rouge">w</code> jump too hard and oscillate around the true value</li>
  <li>The intermediate <em>learning rate</em> 0.001 seems to be a good compromise, taking a bit more time but then stabilizing <code class="language-plaintext highlighter-rouge">w</code>.</li>
</ul>

<h3 id="mini-batch">Mini batch</h3>

<p>The idea behind mini-batches is to take a batch of datapoints at each step instead of going point by point, then averaging the error and the gradient obtained for this mini-batch. This has two advantages:</p>
<ol>
  <li>You can iterate faster through the data: if the size of the mini-batches is <code class="language-plaintext highlighter-rouge">n</code>, then you iterate through the training dataset <code class="language-plaintext highlighter-rouge">n</code> by n instead of 1 by 1.</li>
  <li>The gradient descent is smoother, since the oscillation (loss and gradient value) of the datapoints in the mini-batches are averaged. On the other hand, taking too big mini-batches would “drown” too much the strongest signals and hinder the gradient descent. Let’s implement it:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">gradient_descent_linreg_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> 
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                <span class="n">n_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">ws</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inds</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(((</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">ws</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s">'w'</span><span class="p">:</span> <span class="n">ws</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">:</span> <span class="n">losses</span><span class="p">}</span>


<span class="n">descents</span> <span class="o">=</span> <span class="p">{</span><span class="n">lr</span><span class="p">:</span> <span class="n">gradient_descent_linreg_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> \ 
                                                <span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> \
                <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="p">[.</span><span class="mi">0001</span><span class="p">,</span> <span class="p">.</span><span class="mi">001</span><span class="p">,</span> <span class="p">.</span><span class="mi">01</span><span class="p">]}</span>
<span class="n">plot_descents_lr</span><span class="p">(</span><span class="n">descents</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="assets/gradient-descent-by-batch-size.png" alt="gradient-descent-by-batch-size" title="parameter value w and loss by step for various learning rates with mini-batching" /></p>

<p>Here we can see that mini-batching makes the gradient descent slower, but more stable. For example with the highest learning rate 0.01, the parameter value oscillates less around the true value when it reaches it than previously.</p>

<h2 id="momentum">Momentum</h2>

<p>The <em>momentum</em> variant of the gradient descent consists in replacing the <em>gradient</em> by the <em>momentum gradient</em> in the parameter update formula:</p>

\[w \leftarrow w - lr.m\_gradient\]

<p>The momentum gradient \(m\_gradient\) is defined iteratively as such:</p>

\[m\_gradient \leftarrow gradient(t) - \alpha.m\_gradient(t-1)\]

<p>where \(α\) is the <em>momentum coefficient</em> and \(gradient(t)\) is the gradient calculated at the current step \(t\) from the <em>learning rate</em> and the <em>loss</em>, as we did previously.</p>

<p>To realize why it’s called momentum, let’s expand this formula:</p>

\[\begin{split}
m\_gradient &amp;= gradient(t) + \alpha.m\_gradient(t-1) \\
    &amp;= gradient(t) + \alpha.gradient(t-1) + \alpha^2.m\_gradient(t-2) \\
    &amp;= \dots \\
    &amp;= \sum_{i=0}^{n}\alpha^i.gradient(t-1)
\end{split}\]

<p>The sum at the end represents the <em>momentum</em> of the <em>learning rate</em> trajectory until \(t\). The more recent a step, the higher it weights in the <em>momentum</em>. The higher \(α\), the higher the previous step will amount in the current <em>momentum gradient</em>, hence the higher the <em>momentum</em>.</p>

<p>To illustrate this, let’s implement a function taking into account <em>batch size</em> and <em>momentum</em>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">gradient_descent_linreg_batch_momentum</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> 
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">ws</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">previous_grad</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inds</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(((</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># momentum implemented here:
</span>        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> \
                    <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">previous_grad</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">ws</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">previous_grad</span> <span class="o">=</span> <span class="n">grad</span>
    <span class="k">return</span> <span class="p">{</span><span class="s">'w'</span><span class="p">:</span> <span class="n">ws</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">:</span> <span class="n">losses</span><span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now we take the second example of before with <code class="language-plaintext highlighter-rouge">lr=0.0001</code> and <code class="language-plaintext highlighter-rouge">batch_size=5</code> and add to it it different values for the momentum coefficient \(α\):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="rouge-code"><pre><span class="n">descents</span> <span class="o">=</span> <span class="p">{</span><span class="n">alpha_</span><span class="p">:</span> 
            <span class="n">gradient_descent_linreg_batch_momentum</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> 
                <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_</span><span class="p">)</span> \ 
    <span class="k">for</span> <span class="n">alpha_</span> <span class="ow">in</span> <span class="p">[.</span><span class="mi">0001</span><span class="p">,</span> <span class="p">.</span><span class="mi">001</span><span class="p">,</span> <span class="p">.</span><span class="mi">01</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">plot_descents_momentum</span><span class="p">(</span><span class="n">descents</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> 
                            <span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'Blues'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">descents</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">descents</span><span class="p">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">))),</span> <span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">),</span> 
                    <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'w'</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">descents</span><span class="p">.</span><span class="n">values</span><span class="p">()])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'True Value'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'iteration'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'w'</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'Estimation of w by Iteration for Various'</span>\
             <span class="s">'Momentums</span><span class="se">\n</span><span class="s">learning rate={lr}'</span>
    <span class="k">if</span> <span class="n">batch_size</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s">, batch size = </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">'</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">descents</span><span class="p">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'loss'</span><span class="p">))),</span> 
                    <span class="n">res</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'loss'</span><span class="p">),</span> 
                    <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> 
                    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'iteration'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'w'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'Estimation'</span><span class="p">,</span> <span class="s">'Loss'</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_descents_momentum</span><span class="p">(</span><span class="n">descents</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="assets/gradient-descent-by-momentum.png" alt="gradient-descent-by-momentum" title="Effect of various momentums on gradient descent with low learning rate" /></p>

<p>A higher momentum has a further regularization effect on the loss during the training. On the right plot you can see for example that the loss at the beginning doesn’t peak as high at the beginning with a higher momentum coefficient \(α\). The influence on the convergence speed is however quite modest, as you can see on the left. Why is a smoothy loss (hence regularization) relevant? Because as the loss conditions (together with the learning rate) the amplitude of the correction in the model, a too strong loss due to irregularity can make your model miss the convergence to a local optimum, especially in highly dimensional parameter spaces. Still, the effect of the momentum is not as strong as for the learning rate. It’s for a reason that <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Bengio</a> considered in his <a href="https://arxiv.org/abs/1206.5533">recommendations</a> that the <em>learning rate</em> is the most important hyper-parameter in a neural network.</p>

<p>There are variants of this <em>momentum</em> approach such as <a href="https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a">RMSProp</a> (used for example by <a href="https://deepmind.com/">Google Deepmind</a>) where the <em>momentum</em> is increased in the direction where the slope is the strongest, so that the parameter focuses on the direction improving it most without moving around it.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Gradient descend is an ubiquitous technique in machine learning, especially in deep learning. It’s often very helpful to keep in mind what happens behind the scene when gradient descent is used to train a model. It helps to evaluate what happens and ponder the different options to better train the model.</p>

<p>This article aims to give a first approach to this technique on a simple example. When training model with high dimensional parameters, other phenomena related to the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a> can occur that we barely touched here. Bear also in mind that in a complex model, there can be many locals minima. Considering how to adjust the gradient descent in order to be able to leave “bad” minima (too shallow/narrow) is also important in this case.</p>

<p>When training a neural network, the piece in charge of managing the gradient descent is called the optimizer. There are for example different kind of <a href="https://keras.io/api/optimizers/">optimizers in Keras</a> and other machine learning frameworks, you can also define your own. We hope this article can help you visualizing and understanding what they do.</p>

<p>In general, implementing a method on a simple toy example (as we did here with the 1-dimensional linear regression) really helps grasping the main ideas behind it.</p>

      <hr>
      <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<div id="share-bar">
  <h4>Share this:</h4>
  <div class="share-buttons">
    <a href="https://www.facebook.com/sharer/sharer.php?u=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><em class="fa fa-facebook-official share-button"> facebook</em></a>
    <a href="https://twitter.com/intent/tweet?text=Gradient Descent Explained&url=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><em class="fa fa-twitter share-button"> twitter</em></a>
    <a href="https://plus.google.com/share?url=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google+"><em class="fa fa-google-plus share-button"> google</em></a>
    <a href="http://pinterest.com/pin/create/button/?url=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Pinterest"><em class="fa fa-pinterest-p share-button"> pinterest</em></a>
    <a href="http://www.tumblr.com/share/link?url=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Tumblr"><em class="fa fa-tumblr share-button"> tumblr</em></a>
    <a href="http://www.reddit.com/submit?url=https://mancap314.github.io/gradient-descent-principles.html" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Reddit"><em class="fa fa-reddit-alien share-button"> reddit</em></a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://mancap314.github.io/gradient-descent-principles.html&title=Gradient Descent Explained&summary=&source=Data science and other stuffs" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><em class="fa fa-linkedin share-button"> linkedin</em></a>
    <a href="mailto:?subject=Gradient Descent Explained&amp;body=Check out this site https://mancap314.github.io/gradient-descent-principles.html" title="Share via Email"><em class="fa fa-envelope share-button"> email</em></a>
  </div>
</div>

      <hr>
      
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://mancap314.github.io/gradient-descent-principles.html';
      this.page.identifier = 'https://mancap314.github.io/gradient-descent-principles.html';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://mc-data.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


    </div>
  </body>
</html>
